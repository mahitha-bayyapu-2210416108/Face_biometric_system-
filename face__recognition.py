# -*- coding: utf-8 -*-
"""Face _recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JiRJPoD7xJk07aFIPMrnBRWLsC4u8J1p

# Project on “Analyzing and Improving a Basic Face Biometric System”

Objective: Two data files ProbeSet.rar and GallerySet.rar are provided where the former contains 200
normalized face images from 100 individuals (two per subject) and the latter contains 100 images from
the same 100 individual. In this project, you first develop a very basic face biometric system which
considers the raw pixel intensity values as features as well as a normalized correlation coefficient as
comparator (same as HW3). Next, you will propose and implement a method for improving/attempting-
to-improve the performance of the above basic face biometric system

## Part I:
You build the basic face biomeyric system in this part using the given data “ProbeSet.rar” and
“GallerySet.rar” from HW3. Complete the following:

a) Consider the “normalized correlation coefficient” function (see Appendix) as your similarity
measure and perform the comparison between each probe template with all the gallery
templates to generate a score matrix. Your score matrix A will be a 200x100 matrix where A[i,j]
denotes the match score generated by comparing the i-th probe with the j-th gallery data.
Provide a snippet of the A[0:9,0:9] matrix. (5 points)
"""

# !pip install -q mtcnn

!pip install -q natsort

!pip install -q patool

import numpy as np
import os
from pathlib import Path
import patoolib
import shutil
import cv2
import glob
from natsort import natsorted
import pandas as pd
import matplotlib.pyplot as plt

# # Set the paths of the directories to be deleted
# gallery_set_path = '/content/GallerySet'
# probe_set_path = '/content/ProbeSet'

# # Function to delete non-empty directory
# def delete_directory(directory_path):
#     try:
#         shutil.rmtree(directory_path)
#         print(f"Directory '{directory_path}' deleted successfully.")
#     except Exception as e:
#         print(f"Error deleting directory '{directory_path}': {e}")

# delete_directory(probe_set_path)
# delete_directory(gallery_set_path)

def load_images_and_sort_paths(gallery_set_path, probe_set_path):
    probe_set_read = []
    gallery_set_read = []

    for image in glob.glob(probe_set_path):
        probe_set_read.append(image)

    probe_set = natsorted(probe_set_read)

    for images in glob.glob(gallery_set_path):
        gallery_set_read.append(images)

    gallery_set = natsorted(gallery_set_read)

    probe = [cv2.imread(images, cv2.IMREAD_COLOR) for images in probe_set]
    gallery = [cv2.imread(images, cv2.IMREAD_COLOR) for images in gallery_set]

    gallery = np.array(gallery)
    probe = np.array(probe)

    return gallery, probe

def extract_and_load_rars(rar_paths, extraction_path, gallery_set_folder, probe_set_folder):
    # Create extraction folder if it doesn't exist
    Path(extraction_path).mkdir(parents=True, exist_ok=True)

    extracted_folders = []

    # Extract each RAR file
    for rar_path in rar_paths:
        # Get the name of the extracted folder (assuming it's the same as the RAR file without the extension)
        extracted_folder_name = os.path.splitext(os.path.basename(rar_path))[0]
        extracted_folder_path = os.path.join(extraction_path, extracted_folder_name)

        # Create a subfolder for each RAR file inside the extraction path
        Path(extracted_folder_path).mkdir(parents=True, exist_ok=True)

        patoolib.extract_archive(rar_path, outdir=extracted_folder_path)
        extracted_folders.append(extracted_folder_path)

    # Load images and sort paths using your existing function
    gallery_set_path = os.path.join(extracted_folders[0], gallery_set_folder)
    probe_set_path = os.path.join(extracted_folders[1], probe_set_folder)

    gallery, probe = load_images_and_sort_paths(gallery_set_path + "/*", probe_set_path + "/*")

    return gallery, probe

rar_paths = ["/content/GallerySet.rar", "/content/ProbeSet.rar"]  # Update with the paths to your RAR files
extraction_path = "/content/Extraction"
gallery_set_folder = "GallerySet"
probe_set_folder = "ProbeSet"

gallery, probe = extract_and_load_rars(rar_paths, extraction_path, gallery_set_folder, probe_set_folder)

gallery_set_path = "/content/Extraction/GallerySet/*"
probe_set_path = "/content/Extraction/ProbeSet/*"
gallery, probe = load_images_and_sort_paths(gallery_set_path, probe_set_path)

print('Length of gallery set:', len(gallery))
print('Lenght of probe set:', len(probe))

# Function to calculate the normalized correlation coefficient
def normalizedCorrelationCoefficient(img1, img2):
    x = img1.reshape((-1, 1))
    y = img2.reshape((-1, 1))
    xn = x - np.mean(x)
    yn = y - np.mean(y)
    r = (np.sum(xn * yn)) / (np.sqrt(np.sum(xn**2)) * np.sqrt(np.sum(yn**2)))
    return r

matrix = np.zeros([200,100])

for row in range(200):
    for col in range(100):
        matrix[row, col] = normalizedCorrelationCoefficient(probe[row], gallery[col])

df_mat = pd.DataFrame(matrix)
df_mat.shape

df_mat.iloc[0:9,0:9]

"""(b) Plot the genuine and impostor score distributions in a reasonably comparable window. (5
points)
"""

def extract_score(mat):
    genuine_scores = []
    imposter_scores = []

    # for every col, we want to check for row, row+1
    # when col =0 -> row =0 or row = 1
    for col in range(100):
        for row in range(200):
            if row == 2 * col or row == 2 * col + 1:
                genuine_scores.append(mat[row][col])
            else:
                imposter_scores.append(mat[row][col])

    return np.array(genuine_scores), np.array(imposter_scores)

mat_arr = np.array(df_mat)
genuine_scores, imposter_scores = extract_score(mat_arr)

print(genuine_scores)

print(imposter_scores)

def plot_distribution(genuine, imposter, genuine_color='green', imposter_color='blue'):
    fig, ax = plt.subplots(figsize=(10, 10))

    min_value = min(min(genuine), min(imposter))
    max_value = max(max(genuine), max(imposter))

    ax.hist(genuine, bins=20, alpha=0.5, label='Genuine Scores', color=genuine_color, density=True)
    ax.hist(imposter, bins=15, alpha=0.5, label='Impostor Scores', color=imposter_color, density=True)

    # Set the x-axis limits
    ax.set_xlim([min_value, max_value])

    ax.set_xlabel('Normalized Correlation Coefficient')
    ax.set_ylabel('Probability Density')
    ax.legend()
    ax.set_title('Genuine and Impostor Score Distributions')

    return fig, ax

fig, ax = plot_distribution(genuine_scores, imposter_scores, genuine_color='green', imposter_color='blue')
plt.tight_layout()
plt.show()

"""(c) Report the decidability index value. (5 points)"""

meu_gen = np.mean(genuine_scores)
meu_imp = np.mean(imposter_scores)

sigma_gen = np.std(genuine_scores)
sigma_imp = np.std(imposter_scores)

d_value = abs(meu_gen - meu_imp) / np.sqrt(0.5 * (sigma_gen** 2 + sigma_imp** 2))

print("Decidability Index (d'): {:.4f}".format(d_value))



"""(d) Plot the Receiver Operating Curve (FAR vs. FRR). (5 points)"""

def calculate_FAR_and_FRR(imposter_scores, genuine_scores, num_thresholds):
    thresholds = np.linspace(0, 1, num_thresholds)

    FAR = [sum(1 for i in imposter_scores if i > threshold) / len(imposter_scores) for threshold in thresholds]
    FRR = [sum(1 for i in genuine_scores if i < threshold) / len(genuine_scores) for threshold in thresholds]

    return np.array(FAR), np.array(FRR)

FAR, FRR = calculate_FAR_and_FRR(imposter_scores, genuine_scores, 100)

# Plot the Receiver Operating Characteristics (ROC) Curve
plt.figure(figsize=(10, 10))
plt.plot(FAR, FRR, color='b', linewidth=3)
plt.title('Receiver Operating Characteristics Curve')
plt.ylabel('False Reject Rate')
plt.xlabel('False Accept Rate')
plt.show()







"""(e) What is the EER? (5 points)"""

def calculate_equal_error_rate(false_acceptance_rates, false_reject_rates):
    min_difference = 1.0
    eer_threshold = None
    threshold_values = np.linspace(0, 1, len(false_reject_rates))

    for i in range(len(false_reject_rates)):
        difference = abs(false_reject_rates[i] - false_acceptance_rates[i])
        if difference < min_difference:
            min_difference = difference
            eer_threshold = threshold_values[i]

    return eer_threshold, min_difference

eer_threshold, eer_value = calculate_equal_error_rate(FAR, FRR)

print("Equal Error Rate (EER) Threshold: ", eer_threshold)
print("Minimum EER Difference: ", eer_value)
print("FAR at EER Threshold: ", FAR[int(eer_threshold * (len(FAR) - 1))])
print("FRR at EER Threshold: ", FRR[int(eer_threshold * (len(FRR) - 1))])







"""# Part II:

Part I represents a very basic face biometric system which considers the raw pixel intensity values as
features as well as a normalized correlation coefficient as comparator. As mentioned in the Objective, in
Part II you will propose and implement a method for improving/attempting-to-improve the performance
of the above basic face biometric system. [Hint: You may consider different feature representations,
pre-processing techniques, matching algorithms, etc. or combination of them.]
"""

from skimage.io import imread
from skimage import color
from scipy.ndimage import convolve
from scipy.ndimage import gaussian_filter
from scipy.ndimage import sobel
from scipy.ndimage import generic_filter
from scipy.ndimage import label

def apply_laplacian(img):
    # Convert to gray scale
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply Gaussian Blur
    img_blur = cv2.GaussianBlur(img_gray, (3, 3), 0)

    # Apply Laplacian filter
    laplacian = cv2.Laplacian(img_blur, cv2.CV_64F)

    # Convert back to uint8
    laplacian = np.uint8(np.absolute(laplacian))

    return laplacian

def apply_canny(img, low_threshold, high_threshold):
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 0)
    edges = cv2.Canny(img_blur, low_threshold, high_threshold)
    return edges

# Function to apply Sobel filtering
def apply_sobel(img):
    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5)  # x
    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5)  # y
    sobelxy = cv2.Sobel(src=img, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5)
    return sobelxy

# Function to apply Prewitt filtering
def apply_prewitt(img):
    kernel_x = cv2.getDerivKernels(1, 0, 3, normalize=True)
    kernel_y = cv2.getDerivKernels(0, 1, 3, normalize=True)
    prewitt_x = cv2.filter2D(img, cv2.CV_64F, kernel_x[0] * kernel_x[1].T)
    prewitt_y = cv2.filter2D(img, cv2.CV_64F, kernel_y[0] * kernel_y[1].T)
    prewitt_xy = prewitt_x + prewitt_y
    return prewitt_xy

# Function for filterless processing
def process_image_filterless(image):
    rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
    roi = image[0:25, :]
    return roi

# Function to process an image with optional filtering
def process_image(image, sobel=True, canny=False, prewitt=False, canny_low_threshold=50, canny_high_threshold=150, filterless=False):
    if filterless:
        return process_image_filterless(image)

    rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
    roi = image[0:25, :]
    img_blur = cv2.GaussianBlur(roi, (3, 3), 0)

    processed_image = None

    if sobel:
        img_sobel = apply_sobel(img_blur)
        processed_image = img_sobel

    if canny:
        img_canny = apply_canny(img_blur, canny_low_threshold, canny_high_threshold)
        processed_image = img_canny

    if prewitt:
        img_prewitt = apply_prewitt(img_blur)
        processed_image = img_prewitt

    return processed_image

def process_images_and_calculate_coefficients(filter_function):
    k = 0
    normalized_pg_value = np.zeros((200, 100))

    for i in range(1, 101):
        # Process gallery image
        gallery_file_name = f'/content/Extraction/GallerySet/subject{i}_img1.pgm'
        gallery_image = imread(gallery_file_name, as_gray=True)
        gallery_image_filtered = process_image(gallery_image)

        l = 0
        for j in range(1, 101):
            # Process probe image 2
            probe_file_2_name = f'/content/Extraction/ProbeSet/subject{j}_img2.pgm'
            probe_image_2 = imread(probe_file_2_name, as_gray=True)
            probe_image_2_filtered = process_image(probe_image_2)

            # Process probe image 3
            probe_file_3_name = f'/content/Extraction/ProbeSet/subject{j}_img3.pgm'
            probe_image_3 = imread(probe_file_3_name, as_gray=True)
            probe_image_3_filtered = process_image(probe_image_3)

            # Calculate normalized correlation coefficients
            normalized_pg_value_p2 = normalizedCorrelationCoefficient(gallery_image_filtered, probe_image_2_filtered)
            normalized_pg_value_p3 = normalizedCorrelationCoefficient(gallery_image_filtered, probe_image_3_filtered)

            # Store the values in the result matrix
            normalized_pg_value[l, k] = normalized_pg_value_p2
            normalized_pg_value[l + 1, k] = normalized_pg_value_p3

            if l < 198:
                l += 2

        k += 1

    # Create a Pandas DataFrame from the result matrix
    columns = [f'Probe{j}_Gallery{k}' for k in range(1, 101)]
    index = [f'Match{i}' for i in range(1, 201)]
    result_df = pd.DataFrame(normalized_pg_value, columns=columns, index=index)

    return result_df

result_sobel = process_images_and_calculate_coefficients(apply_sobel)

result_sobel.iloc[0:9,0:9]

"""### Testing with Face recognition"""

# # Import required libraries
# import cv2
# import numpy as np
# from skimage.io import imread
# from skimage.feature import local_binary_pattern
# from scipy.stats import zscore

# # Function to process image using Local Binary Pattern
# def process_image(img):
#     radius = 3
#     n_points = 8 * radius
#     lbp = local_binary_pattern(img, n_points, radius, method='uniform')
#     return lbp.astype(np.uint8)

# # Load images and process gallery images for LBPH training
# gallerySet = []
# for i in range(1, 101):
#     img = cv2.imread(f"/content/Extraction/GallerySet/subject{i}_img1.pgm", cv2.IMREAD_GRAYSCALE)
#     img = cv2.equalizeHist(img)
#     # img = cv2.normalize(img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)
#     gallerySet.append(img)

# # Training the Face Recognition Model using Local Binary Pattern Histogram (LBPH)
# FaceRecognizer = cv2.face.LBPHFaceRecognizer_create()
# FaceRecognizer.train(gallerySet, np.array(range(1, 101)))

# # Main function to iterate over gallery and probe images and calculate LBPH confidence scores
# def process_images_and_calculate_confidence():
#     k = 0
#     confidence_scores = np.zeros((200, 100))

#     for i in range(1, 101):
#         # Process gallery image for LBPH
#         gallery_file_name = f'/content/Extraction/GallerySet/subject{i}_img1.pgm'
#         gallery_image = cv2.imread(gallery_file_name, cv2.IMREAD_GRAYSCALE)
#         _, gallery_image_bin = cv2.threshold(gallery_image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
#         gallery_confidence, _ = FaceRecognizer.predict(gallery_image_bin)

#         l = 0
#         for j in range(1, 101):
#             # Process probe image 2
#             probe_file_2_name = f'/content/Extraction/ProbeSet/subject{j}_img2.pgm'
#             probe_image_2 = cv2.imread(probe_file_2_name, cv2.IMREAD_GRAYSCALE)
#             _, probe_image_2_bin = cv2.threshold(probe_image_2, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
#             probe_confidence_2, _ = FaceRecognizer.predict(probe_image_2_bin)

#             # Process probe image 3
#             probe_file_3_name = f'/content/Extraction/ProbeSet/subject{j}_img3.pgm'
#             probe_image_3 = cv2.imread(probe_file_3_name, cv2.IMREAD_GRAYSCALE)
#             _, probe_image_3_bin = cv2.threshold(probe_image_3, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
#             probe_confidence_3, _ = FaceRecognizer.predict(probe_image_3_bin)

#             # Store the confidence scores in the result matrix
#             confidence_scores[l, k] = abs(gallery_confidence - probe_confidence_2)
#             confidence_scores[l + 1, k] = abs(gallery_confidence - probe_confidence_3)

#             if l < 198:
#                 l += 2

#         k += 1

#     # Create a Pandas DataFrame from the result matrix
#     columns = [f'Probe{j}_Gallery{k}' for k in range(1, 101)]
#     index = [f'Match{i}' for i in range(1, 201)]
#     result_df = pd.DataFrame(confidence_scores, columns=columns, index=index)

#     return result_df

# #Test2

# # Main function to iterate over gallery and probe images and calculate LBPH confidence scores
# def process_images_and_calculate_confidence(FaceRecognizer):
#     k = 0
#     confidence_scores = np.zeros((200, 100))

#     for i in range(1, 101):
#         # Process gallery image for LBPH
#         gallery_file_name = f'/content/Extraction/GallerySet/subject{i}_img1.pgm'
#         gallery_image = cv2.imread(gallery_file_name, cv2.IMREAD_GRAYSCALE)
#         _, gallery_image_bin = cv2.threshold(gallery_image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
#         gallery_confidence, _ = FaceRecognizer.predict(gallery_image_bin)

#         l = 0
#         for j in range(1, 101):
#             # Process probe image 2
#             probe_file_2_name = f'/content/Extraction/ProbeSet/subject{j}_img2.pgm'
#             probe_image_2 = cv2.imread(probe_file_2_name, cv2.IMREAD_GRAYSCALE)
#             _, probe_image_2_bin = cv2.threshold(probe_image_2, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
#             probe_confidence_2, _ = FaceRecognizer.predict(probe_image_2_bin)

#             # Process probe image 3
#             probe_file_3_name = f'/content/Extraction/ProbeSet/subject{j}_img3.pgm'
#             probe_image_3 = cv2.imread(probe_file_3_name, cv2.IMREAD_GRAYSCALE)
#             _, probe_image_3_bin = cv2.threshold(probe_image_3, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
#             probe_confidence_3, _ = FaceRecognizer.predict(probe_image_3_bin)

#             # Store the confidence scores in the result matrix
#             # Use abs() to avoid negative values
#             confidence_scores[l, k] = abs(gallery_confidence - probe_confidence_2)
#             confidence_scores[l + 1, k] = abs(gallery_confidence - probe_confidence_3)

#             if l < 198:
#                 l += 2

#         k += 1

#     # Create a Pandas DataFrame from the result matrix
#     columns = [f'Probe{j}_Gallery{k}' for k in range(1, 101)]
#     index = [f'Match{i}' for i in range(1, 201)]
#     result_df = pd.DataFrame(confidence_scores, columns=columns, index=index)

#     return result_df

# result_matrix_df = process_images_and_calculate_confidence()
#result_matrix_df = process_images_and_calculate_coefficients()

# result_matrix_df.iloc[0:9, 0:9]

"""### Score distribution graph"""

def plot_score_distributions(matrix_df):
    genuine = []
    imposter = []

    for i in range(0, 100):
        for j in range(0, 200):
            if j == (i * 2):
                genuine.append(matrix_df.iloc[j, i])
            elif j == (i * 2) + 1:
                genuine.append(matrix_df.iloc[j, i])
            else:
                imposter.append(matrix_df.iloc[j, i])

    # Plotting genuine and impostor score distributions
    plt.figure(figsize=(8, 6))
    plt.hist(genuine, density=True, stacked=True, color='b', label='Genuine')
    plt.hist(imposter, density=True, stacked=True, color='r', label='Imposter')
    plt.title('Genuine and Impostor Score Distributions')
    plt.xlabel('Score')
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.legend()
    plt.show()

plot_score_distributions(result_sobel)

def extract_genuine_imposter_scores(matrix_df):
        genuine = []
        imposter = []

        for i in range(0, 100):
            for j in range(0, 200):
                if j == (i * 2):
                    genuine.append(matrix_df.iloc[j, i])
                elif j == (i * 2) + 1:
                    genuine.append(matrix_df.iloc[j, i])
                else:
                    imposter.append(matrix_df.iloc[j, i])

        return genuine, imposter

genuine_scores_II, imposter_scores_II = extract_genuine_imposter_scores(result_sobel)

"""### Scores Testing with face *recognition*"""

# probeSet = []
# for i in range(1, 201):
#     # Read images
#     img2 = cv2.imread("/content/Extraction/ProbeSet/subject"+str(i)+"_img2.pgm", cv2.IMREAD_GRAYSCALE)
#     # Add image normalization
#     img2 = cv2.normalize(img2, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)
#     # Binarize images using Otsu's thresholding
#     Threshold_Value, img2_bin = cv2.threshold(img2, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
#     probeSet.append(img2_bin)

# def extract_genuine_imposter_scores_using_facerecognition(FaceRecognizer, probeSet, gallerySet,matrix_df):
#     genuine = []
#     imposter = []

#     for i in range(100):
#         for j in range(200):
#             if j == (i * 2):
#                 # Compute the confidence score for genuine matches using the trained face recognition model
#                 label, confidence = FaceRecognizer.predict(probeSet[i])
#                 genuine.append(confidence)
#             elif j == (i * 2) + 1:
#                 # Compute the confidence score for genuine matches using the trained face recognition model
#                 label, confidence = FaceRecognizer.predict(probeSet[i])
#                 genuine.append(confidence)
#             else:
#                 # Compute the confidence score for imposter matches using the trained face recognition model
#                 imposter.append(matrix_df.iloc[j, i])


#     return genuine, imposter

# # Usage
# genuine_scores_FR, imposter_scores_FR = extract_genuine_imposter_scores_using_facerecognition(FaceRecognizer, probeSet, gallerySet,result_matrix_df)

def calculate_d_prime(genuine_scores, imposter_scores):
    d_prime = (np.sqrt(2) * (np.abs(np.mean(genuine_scores) - np.mean(imposter_scores)))) / (
            np.sqrt(np.std(genuine_scores) ** 2 + np.std(imposter_scores) ** 2))
    return d_prime

d_prime_value = calculate_d_prime(genuine_scores_II, imposter_scores_II)
print("The D' value of system II is: ", d_prime_value)

def calculate_FAR_and_FRR(imposter_scores, genuine_scores, num_thresholds):
    thresholds = np.linspace(0, 1, num_thresholds)

    FAR = [sum(1 for i in imposter_scores if i > threshold) / len(imposter_scores) for threshold in thresholds]
    FRR = [sum(1 for i in genuine_scores if i < threshold) / len(genuine_scores) for threshold in thresholds]

    return np.array(FAR), np.array(FRR)

FAR_II, FRR_II = calculate_FAR_and_FRR(imposter_scores_II, genuine_scores_II, 100)

# Plot the Receiver Operating Characteristics (ROC) Curve
plt.figure(figsize=(8,6))
plt.plot(FAR_II, FRR_II, color='b', linewidth=3)
plt.title('Receiver Operating Characteristics Curve')
plt.ylabel('False Reject Rate')
plt.xlabel('False Accept Rate')
plt.show()

"""### Testing FAR with Face Recognition"""

# def calculate_FAR_and_FRR(imposter_scores, genuine_scores, num_thresholds):
#     thresholds = np.linspace(0, 1, num_thresholds)

#     FAR = [sum(1 for i in imposter_scores if i >= threshold) / len(imposter_scores) for threshold in thresholds]
#     FRR = [sum(1 for i in genuine_scores if i < threshold) / len(genuine_scores) for threshold in thresholds]

#     return np.array(FAR), np.array(FRR)

# # Usage
# FAR_II, FRR_II = calculate_FAR_and_FRR(imposter_scores_FR, genuine_scores_FR, 100)

# # Plot the Receiver Operating Characteristics (ROC) Curve
# plt.figure(figsize=(8, 6))
# plt.plot(FAR_II, FRR_II, color='b', linewidth=3)
# plt.title('Receiver Operating Characteristics Curve')
# plt.ylabel('False Reject Rate')
# plt.xlabel('False Accept Rate')
# plt.show()

eer_threshold_II, eer_value_II = calculate_equal_error_rate(FAR, FRR)

print("Equal Error Rate (EER) Threshold: ", eer_threshold_II)
print("Minimum EER Difference: ", eer_value_II)
print("FAR at EER Threshold: ", FAR_II[int(eer_threshold_II * (len(FAR_II) - 1))])
print("FRR at EER Threshold: ", FRR_II[int(eer_threshold_II * (len(FRR_II) - 1))])

"""The goal of Part II is to revise the basic system in such a way so that it can provide better
recognition performance. Quantify the amount of improvement by the metric called
“Improvement Factor (IF)” considering the given formula. (20 points)
IF = round (delta_ d’, 2) where delta_d’ = d’ of your proposed system – d’ of basic system
"""

DeltaDPrime = d_prime_value - d_value
ImprovementFactor = round(DeltaDPrime, 2)
print('Improvement factor:', ImprovementFactor)